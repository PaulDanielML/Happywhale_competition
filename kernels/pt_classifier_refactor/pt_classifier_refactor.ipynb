{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/paul/projects/Happywhale_competition/data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "ON_KAGGLE_KERNEL = os.path.isdir(\"/kaggle/input\")\n",
    "start_dir = os.getcwd()\n",
    "\n",
    "if ON_KAGGLE_KERNEL:\n",
    "    os.chdir(\"/kaggle/input/utilities/\")\n",
    "else:\n",
    "    os.chdir(f\"{os.environ.get('PYTHONPATH')}/src/utils\")\n",
    "\n",
    "from common import load_structure, save_structure, load_train_file, INPUT_DATA_DIR, OUTPUT_DATA_DIR, SUB_DIR, set_seed, create_dir_if_not_exists\n",
    "from data_proc import create_idvs_with_one_img_in_train_split, create_train_val_loaders, split_into_train_val\n",
    "from data_structures import WhaleDataset, TorchConfig\n",
    "os.chdir(start_dir)\n",
    "\n",
    "print(INPUT_DATA_DIR)\n",
    "\n",
    "if not ON_KAGGLE_KERNEL:\n",
    "    %reload_ext autoreload\n",
    "    %autoreload 2\n",
    "    from IPython.core.interactiveshell import InteractiveShell\n",
    "    InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import time\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "try:\n",
    "    import timm\n",
    "except Exception:\n",
    "    !pip install --upgrade timm\n",
    "    import timm\n",
    "try:\n",
    "    import wandb\n",
    "except Exception:\n",
    "    !pip install --upgrade wandb\n",
    "    import wandb\n",
    "\n",
    "create_dir_if_not_exists(OUTPUT_DATA_DIR)\n",
    "create_dir_if_not_exists(SUB_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ON_KAGGLE_KERNEL:\n",
    "    try:\n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "        user_secrets = UserSecretsClient()\n",
    "        api_key = user_secrets.get_secret(\"wandb_api\")\n",
    "        wandb.login(key=api_key)\n",
    "        wandb_active = True\n",
    "    except:\n",
    "        wandb_active = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seed': 319,\n",
       " 'epochs': 10,\n",
       " 'img_size': 448,\n",
       " 'augm_args': {'hor_flip': {'p': 0.5},\n",
       "  'ver_flip': {'p': 0.5},\n",
       "  'rot': {'p': 0.5, 'limit': 30}},\n",
       " 'model_name': 'tf_efficientnet_b0',\n",
       " 'num_classes': 15587,\n",
       " 'train_batch_size': 32,\n",
       " 'valid_batch_size': 64,\n",
       " 'optim': torch.optim.adam.Adam,\n",
       " 'optim_args': {'lr': 0.0001, 'weight_decay': 1e-06},\n",
       " 'scheduler': torch.optim.lr_scheduler.CosineAnnealingLR,\n",
       " 'scheduler_args': {'T_max': 500, 'eta_min': 1e-06},\n",
       " 'n_fold': 5,\n",
       " 'init_optim_': None,\n",
       " 'init_sched_': None}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = TorchConfig.default()\n",
    "conf.dict()\n",
    "set_seed(conf.seed)\n",
    "\n",
    "if not ON_KAGGLE_KERNEL:\n",
    "    conf.train_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding labels of df...\n",
      "Not saving /home/paul/projects/Happywhale_competition/data/id_encoding.pickle!\n",
      "/home/paul/projects/Happywhale_competition/data/splits/idvs_with_one_img_in_train.pickle saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/projects/Happywhale_competition/env/lib/python3.9/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df = create_idvs_with_one_img_in_train_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40589, 10444)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train, df_val = split_into_train_val(df)\n",
    "len(df_train), len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = conf.make_transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = WhaleDataset(df_train, transforms[\"train\"])\n",
    "val_set = WhaleDataset(df_val, transforms[\"valid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = create_train_val_loaders(train_set, val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (conv_stem): Conv2dSame(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "  (bn1): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act1): SiLU(inplace=True)\n",
       "  (blocks): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): DepthwiseSeparableConv(\n",
       "        (conv_dw): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn1): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pw): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2dSame(96, 96, kernel_size=(3, 3), stride=(2, 2), groups=96, bias=False)\n",
       "        (bn2): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "        (bn2): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2dSame(144, 144, kernel_size=(5, 5), stride=(2, 2), groups=144, bias=False)\n",
       "        (bn2): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "        (bn2): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2dSame(240, 240, kernel_size=(3, 3), stride=(2, 2), groups=240, bias=False)\n",
       "        (bn2): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "        (bn2): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "        (bn2): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "        (bn2): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "        (bn2): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "        (bn2): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2dSame(672, 672, kernel_size=(5, 5), stride=(2, 2), groups=672, bias=False)\n",
       "        (bn2): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "        (bn2): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "        (bn2): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "        (bn2): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "        (bn2): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_head): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (bn2): BatchNorm2d(1280, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act2): SiLU(inplace=True)\n",
       "  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
       "  (classifier): Linear(in_features=1280, out_features=15587, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = timm.create_model(conf.model_name, pretrained=True, num_classes=conf.num_classes)\n",
    "model.to(device);\n",
    "MODEL_TRAINED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(outputs, labels):\n",
    "    return nn.CrossEntropyLoss()(outputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = conf.get_optim(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = conf.get_scheduler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for step, data in bar:\n",
    "        images = data['image'].to(device, dtype=torch.float)\n",
    "        labels = data['label'].to(device, dtype=torch.long)\n",
    "        \n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # loss = loss / conf['n_accumulate']\n",
    "            \n",
    "        loss.backward()\n",
    "    \n",
    "        # if (step + 1) % CONFIG['n_accumulate'] == 0:\n",
    "        optimizer.step()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "                \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        \n",
    "        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n",
    "                        LR=optimizer.param_groups[0]['lr'])\n",
    "    gc.collect()\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def valid_one_epoch(model, dataloader, device, epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for step, data in bar:        \n",
    "        images = data['image'].to(device, dtype=torch.float)\n",
    "        labels = data['label'].to(device, dtype=torch.long)\n",
    "        \n",
    "        batch_size = images.size(0)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        \n",
    "        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss)   \n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(model, optimizer, scheduler, num_epochs):\n",
    "    # To automatically log gradients\n",
    "    wandb.watch(model, log_freq=100)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n",
    "\n",
    "    start = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_epoch_loss = np.inf\n",
    "    history = defaultdict(list)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        gc.collect()\n",
    "        train_epoch_loss = train_one_epoch(\n",
    "            model,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            dataloader=train_loader,\n",
    "            device=device,\n",
    "            epoch=epoch,\n",
    "        )\n",
    "\n",
    "        val_epoch_loss = valid_one_epoch(model, val_loader, device=device, epoch=epoch)\n",
    "\n",
    "        history[\"Train Loss\"].append(train_epoch_loss)\n",
    "        history[\"Valid Loss\"].append(val_epoch_loss)\n",
    "\n",
    "        # Log the metrics\n",
    "        wandb.log({\"Train Loss\": train_epoch_loss})\n",
    "        wandb.log({\"Valid Loss\": val_epoch_loss})\n",
    "\n",
    "        # deep copy the model\n",
    "        if val_epoch_loss <= best_epoch_loss:\n",
    "            print(f\"Validation Loss Improved ({best_epoch_loss} ---> {val_epoch_loss})\")\n",
    "            best_epoch_loss = val_epoch_loss\n",
    "            run.summary[\"Best Loss\"] = best_epoch_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            PATH = \"Loss{:.4f}_epoch{:.0f}.bin\".format(best_epoch_loss, epoch)\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "            # Save a model file from the current directory\n",
    "            print(f\"Model Saved.\")\n",
    "\n",
    "        print()\n",
    "\n",
    "    end = time.time()\n",
    "    time_elapsed = end - start\n",
    "    print(\n",
    "        \"Training complete in {:.0f}h {:.0f}m {:.0f}s\".format(\n",
    "            time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60\n",
    "        )\n",
    "    )\n",
    "    print(\"Best Loss: {:.4f}\".format(best_epoch_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpauldanielml\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/pauldanielml/HappyWhale/runs/1cr8n0dh\" target=\"_blank\">good-shadow-9</a></strong> to <a href=\"https://wandb.ai/pauldanielml/HappyWhale\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using GPU: NVIDIA GeForce GTX 1650\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1268 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/paul/projects/Happywhale_competition/env/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/paul/projects/Happywhale_competition/env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    return self.collate_fn(data)\n  File \"/home/paul/projects/Happywhale_competition/env/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 86, in default_collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/paul/projects/Happywhale_competition/kernels/pt_classifier_refactor/pt_classifier_refactor.ipynb Cell 17'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/paul/projects/Happywhale_competition/kernels/pt_classifier_refactor/pt_classifier_refactor.ipynb#ch0000016?line=0'>1</a>\u001b[0m run \u001b[39m=\u001b[39m wandb\u001b[39m.\u001b[39minit(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/paul/projects/Happywhale_competition/kernels/pt_classifier_refactor/pt_classifier_refactor.ipynb#ch0000016?line=1'>2</a>\u001b[0m     project\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mHappyWhale\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/paul/projects/Happywhale_competition/kernels/pt_classifier_refactor/pt_classifier_refactor.ipynb#ch0000016?line=2'>3</a>\u001b[0m     config\u001b[39m=\u001b[39mconf\u001b[39m.\u001b[39mdict(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/paul/projects/Happywhale_competition/kernels/pt_classifier_refactor/pt_classifier_refactor.ipynb#ch0000016?line=5'>6</a>\u001b[0m     anonymous\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmust\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/paul/projects/Happywhale_competition/kernels/pt_classifier_refactor/pt_classifier_refactor.ipynb#ch0000016?line=6'>7</a>\u001b[0m )\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/paul/projects/Happywhale_competition/kernels/pt_classifier_refactor/pt_classifier_refactor.ipynb#ch0000016?line=8'>9</a>\u001b[0m model, history \u001b[39m=\u001b[39m run_training(model, optim, scheduler, num_epochs\u001b[39m=\u001b[39;49mconf\u001b[39m.\u001b[39;49mepochs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/paul/projects/Happywhale_competition/kernels/pt_classifier_refactor/pt_classifier_refactor.ipynb#ch0000016?line=9'>10</a>\u001b[0m MODEL_TRAINED \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/paul/projects/Happywhale_competition/kernels/pt_classifier_refactor/pt_classifier_refactor.ipynb#ch0000016?line=10'>11</a>\u001b[0m run\u001b[39m.\u001b[39mfinish()\n",
      "\u001b[1;32m/home/paul/projects/Happywhale_competition/kernels/pt_classifier_refactor/pt_classifier_refactor.ipynb Cell 16'\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(model, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/paul/projects/Happywhale_competition/kernels/pt_classifier_refactor/pt_classifier_refactor.ipynb#ch0000015?line=12'>13</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, num_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/paul/projects/Happywhale_competition/kernels/pt_classifier_refactor/pt_classifier_refactor.ipynb#ch0000015?line=13'>14</a>\u001b[0m     gc\u001b[39m.\u001b[39mcollect()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/paul/projects/Happywhale_competition/kernels/pt_classifier_refactor/pt_classifier_refactor.ipynb#ch0000015?line=14'>15</a>\u001b[0m     train_epoch_loss \u001b[39m=\u001b[39m train_one_epoch(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/paul/projects/Happywhale_competition/kernels/pt_classifier_refactor/pt_classifier_refactor.ipynb#ch0000015?line=15'>16</a>\u001b[0m         model,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/paul/projects/Happywhale_competition/kernels/pt_classifier_refactor/pt_classifier_refactor.ipynb#ch0000015?line=16'>17</a>\u001b[0m         optimizer,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/paul/projects/Happywhale_competition/kernels/pt_classifier_refactor/pt_classifier_refactor.ipynb#ch0000015?line=17'>18</a>\u001b[0m         scheduler,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/paul/projects/Happywhale_competition/kernels/pt_classifier_refactor/pt_classifier_refactor.ipynb#ch0000015?line=18'>19</a>\u001b[0m         dataloader\u001b[39m=\u001b[39;49mtrain_loader,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/paul/projects/Happywhale_competition/kernels/pt_classifier_refactor/pt_classifier_refactor.ipynb#ch0000015?line=19'>20</a>\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/paul/projects/Happywhale_competition/kernels/pt_classifier_refactor/pt_classifier_refactor.ipynb#ch0000015?line=20'>21</a>\u001b[0m         epoch\u001b[39m=\u001b[39;49mepoch,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/paul/projects/Happywhale_competition/kernels/pt_classifier_refactor/pt_classifier_refactor.ipynb#ch0000015?line=21'>22</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/paul/projects/Happywhale_competition/kernels/pt_classifier_refactor/pt_classifier_refactor.ipynb#ch0000015?line=23'>24</a>\u001b[0m     val_epoch_loss \u001b[39m=\u001b[39m valid_one_epoch(model, val_loader, device\u001b[39m=\u001b[39mdevice, epoch\u001b[39m=\u001b[39mepoch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/paul/projects/Happywhale_competition/kernels/pt_classifier_refactor/pt_classifier_refactor.ipynb#ch0000015?line=25'>26</a>\u001b[0m     history[\u001b[39m\"\u001b[39m\u001b[39mTrain Loss\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mappend(train_epoch_loss)\n",
      "\u001b[1;32m/home/paul/projects/Happywhale_competition/kernels/pt_classifier_refactor/pt_classifier_refactor.ipynb Cell 14'\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, scheduler, dataloader, device, epoch)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/paul/projects/Happywhale_competition/kernels/pt_classifier_refactor/pt_classifier_refactor.ipynb#ch0000013?line=4'>5</a>\u001b[0m running_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/paul/projects/Happywhale_competition/kernels/pt_classifier_refactor/pt_classifier_refactor.ipynb#ch0000013?line=6'>7</a>\u001b[0m bar \u001b[39m=\u001b[39m tqdm(\u001b[39menumerate\u001b[39m(dataloader), total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(dataloader))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/paul/projects/Happywhale_competition/kernels/pt_classifier_refactor/pt_classifier_refactor.ipynb#ch0000013?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m step, data \u001b[39min\u001b[39;00m bar:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/paul/projects/Happywhale_competition/kernels/pt_classifier_refactor/pt_classifier_refactor.ipynb#ch0000013?line=8'>9</a>\u001b[0m     images \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/paul/projects/Happywhale_competition/kernels/pt_classifier_refactor/pt_classifier_refactor.ipynb#ch0000013?line=9'>10</a>\u001b[0m     labels \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\n",
      "File \u001b[0;32m~/projects/Happywhale_competition/env/lib/python3.9/site-packages/tqdm/std.py:1180\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///~/projects/Happywhale_competition/env/lib/python3.9/site-packages/tqdm/std.py?line=1176'>1177</a>\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   <a href='file:///~/projects/Happywhale_competition/env/lib/python3.9/site-packages/tqdm/std.py?line=1178'>1179</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///~/projects/Happywhale_competition/env/lib/python3.9/site-packages/tqdm/std.py?line=1179'>1180</a>\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   <a href='file:///~/projects/Happywhale_competition/env/lib/python3.9/site-packages/tqdm/std.py?line=1180'>1181</a>\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   <a href='file:///~/projects/Happywhale_competition/env/lib/python3.9/site-packages/tqdm/std.py?line=1181'>1182</a>\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/projects/Happywhale_competition/env/lib/python3.9/site-packages/tqdm/std.py?line=1182'>1183</a>\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/Happywhale_competition/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///~/projects/Happywhale_competition/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=518'>519</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///~/projects/Happywhale_competition/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=519'>520</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> <a href='file:///~/projects/Happywhale_competition/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=520'>521</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    <a href='file:///~/projects/Happywhale_competition/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=521'>522</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///~/projects/Happywhale_competition/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=522'>523</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///~/projects/Happywhale_competition/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=523'>524</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///~/projects/Happywhale_competition/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=524'>525</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/projects/Happywhale_competition/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1203\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///~/projects/Happywhale_competition/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=1200'>1201</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///~/projects/Happywhale_competition/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=1201'>1202</a>\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> <a href='file:///~/projects/Happywhale_competition/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=1202'>1203</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m~/projects/Happywhale_competition/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1229\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   <a href='file:///~/projects/Happywhale_competition/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=1226'>1227</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   <a href='file:///~/projects/Happywhale_competition/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=1227'>1228</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> <a href='file:///~/projects/Happywhale_competition/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=1228'>1229</a>\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   <a href='file:///~/projects/Happywhale_competition/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=1229'>1230</a>\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/projects/Happywhale_competition/env/lib/python3.9/site-packages/torch/_utils.py:434\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///~/projects/Happywhale_competition/env/lib/python3.9/site-packages/torch/_utils.py?line=429'>430</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    <a href='file:///~/projects/Happywhale_competition/env/lib/python3.9/site-packages/torch/_utils.py?line=430'>431</a>\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/projects/Happywhale_competition/env/lib/python3.9/site-packages/torch/_utils.py?line=431'>432</a>\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/projects/Happywhale_competition/env/lib/python3.9/site-packages/torch/_utils.py?line=432'>433</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> <a href='file:///~/projects/Happywhale_competition/env/lib/python3.9/site-packages/torch/_utils.py?line=433'>434</a>\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/paul/projects/Happywhale_competition/env/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/paul/projects/Happywhale_competition/env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    return self.collate_fn(data)\n  File \"/home/paul/projects/Happywhale_competition/env/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 86, in default_collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project=\"HappyWhale\",\n",
    "    config=conf.dict(),\n",
    "    job_type=\"Train\",\n",
    "    tags=[\"Standard classifier\", \"efficientnet_b0\", \"448\"],\n",
    "    anonymous=\"must\",\n",
    ")\n",
    "\n",
    "model, history = run_training(model, optim, scheduler, num_epochs=conf.epochs)\n",
    "MODEL_TRAINED = True\n",
    "run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate_predictions_pt(model: nn.Module) -> pd.DataFrame:\n",
    "    df = pd.read_csv(INPUT_DATA_DIR / \"sample_submission.csv\").drop(columns=\"predictions\")\n",
    "    test_dataset = WhaleDataset(df, transforms[\"valid\"], labels=False)\n",
    "    test_loader = DataLoader(dataset=test_dataset, num_workers=4, batch_size=32)\n",
    "\n",
    "    enc = load_structure(\"id_encoding\")\n",
    "\n",
    "    data = []\n",
    "    for batch in test_loader:\n",
    "        imgs = batch[\"image\"].to(device)\n",
    "        res = F.softmax(model(imgs))\n",
    "\n",
    "        top = res.cpu().topk(5)\n",
    "        for idx, val in zip(top.indices.numpy().tolist(), top.values.numpy().tolist()):\n",
    "            ids = enc.inverse_transform(idx).tolist()\n",
    "            data.append(dict(zip(ids, val)))\n",
    "\n",
    "    return_vals = [{\"image\": df.iloc[i][\"image\"], \"top5\": json.dumps(d)} for i,d in enumerate(data)]\n",
    "    return return_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_new_ind(df: pd.DataFrame, softmax_thr: float):\n",
    "    \n",
    "    df_local = df.copy(deep=True)\n",
    "\n",
    "    def insert_new_ind(x, thr: float):\n",
    "        data = json.loads(x)\n",
    "        idvs = list(data.keys())\n",
    "        ret_value = []\n",
    "        for idx, (idv, score) in enumerate(data.items()):\n",
    "            if score > thr:\n",
    "                ret_value.append(idv)\n",
    "            else:\n",
    "                ret_value.append(\"new_individual\")\n",
    "                ret_value += idvs[idx:len(idvs)-1]\n",
    "                break\n",
    "\n",
    "        return \" \".join(ret_value)\n",
    "\n",
    "\n",
    "    df_local[\"predictions\"] = df_local[\"top5\"].apply(lambda x: insert_new_ind(x, softmax_thr))\n",
    "    return df_local.drop(columns=\"top5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.DataFrame(generate_predictions_pt(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_thr = predict_new_ind(pred, 0.04).set_index(\"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_thr.to_csv(SUB_DIR / \"pt_class_single_img_idv_in_train_thr_0,04.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3d5c075392d29132b662e04dfe399f321127320a5d404537ff18030005fe1d27"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
